from omegaconf import OmegaConf
import argparse
import os
import sys
from PIL import Image
import os.path as osp
import numpy as np
import random
import importlib
import json

import torch
import torchvision.transforms as transforms
import cv2
print(">>> Running version from Pose_Estimation_Model")


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
ROOT_DIR = os.path.join(BASE_DIR, '..', 'Pose_Estimation_Model')
sys.path.append(os.path.join(ROOT_DIR, 'provider'))
sys.path.append(os.path.join(ROOT_DIR, 'utils'))
sys.path.append(os.path.join(ROOT_DIR, 'model'))
sys.path.append(os.path.join(BASE_DIR, 'model', 'pointnet2'))


def get_parser():
    parser = argparse.ArgumentParser(
        description="Pose Estimation")
    # pem
    parser.add_argument("--gpus",
                        type=str,
                        default="0",
                        help="path to pretrain model")
    parser.add_argument("--model",
                        type=str,
                        default="pose_estimation_model",
                        help="path to model file")
    parser.add_argument("--config",
                        type=str,
                        default="config/base.yaml",
                        help="path to config file, different config.yaml use different config")
    parser.add_argument("--iter",
                        type=int,
                        default=600000,
                        help="epoch num. for testing")
    parser.add_argument("--exp_id",
                        type=int,
                        default=0,
                        help="")
    
    # input
    parser.add_argument("--output_dir", nargs="?", help="Path to root directory of the output")
    parser.add_argument("--cad_path", nargs="?", help="Path to CAD(mm)")
    parser.add_argument("--rgb_path", nargs="?", help="Path to RGB image")
    parser.add_argument("--depth_path", nargs="?", help="Path to Depth image(mm)")
    parser.add_argument("--cam_path", nargs="?", help="Path to camera information")
    parser.add_argument("--seg_path", nargs="?", help="Path to segmentation information(generated by ISM)")
    parser.add_argument("--det_score_thresh", default=0.2, help="The score threshold of detection")
    parser.add_argument("--frame_idx", type=int, default=0, help="Current frame index")

    args_cfg = parser.parse_args()

    return args_cfg

def init():
    args = get_parser()
    exp_name = args.model + '_' + \
        osp.splitext(args.config.split("/")[-1])[0] + '_id' + str(args.exp_id)
    log_dir = osp.join("log", exp_name)

    #cfg = gorilla.Config.fromfile(args.config)
    cfg = OmegaConf.load(args.config)
    cfg.exp_name = exp_name
    cfg.gpus     = args.gpus
    cfg.model_name = args.model
    cfg.log_dir  = log_dir
    cfg.test_iter = args.iter

    cfg.output_dir = args.output_dir
    cfg.cad_path = args.cad_path
    cfg.rgb_path = args.rgb_path
    cfg.depth_path = args.depth_path
    cfg.cam_path = args.cam_path
    cfg.seg_path = args.seg_path
    #cfg.frame_idx = 0
    cfg.frame_idx = args.frame_idx


    if 'test_dataset' not in cfg:
        cfg.test_dataset = OmegaConf.create()

    if 'frame_idx' not in cfg.test_dataset:
        cfg.test_dataset.frame_idx = 0 

    cfg.det_score_thresh = args.det_score_thresh
    #gorilla.utils.set_cuda_visible_devices(gpu_ids = cfg.gpus)
    os.environ["CUDA_VISIBLE_DEVICES"] = ",".join([str(g) for g in cfg.gpus])

    return  cfg



from data_utils import (
    load_im,
    get_bbox,
    get_point_cloud_from_depth,
    get_resize_rgb_choose,
)
from draw_utils import draw_detections
import pycocotools.mask as cocomask
import trimesh

rgb_transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])])

def visualize(rgb, pred_rot, pred_trans, model_points, K, save_path):
    R = pred_rot[0]  # (3, 3)
    t = pred_trans[0]  # (3,)
    points = model_points.T  # (3, N)

    # Transform model points
    transformed = R @ points + t[:, np.newaxis]  # (3, N)

    # Check Z-values (depth)
    z_vals = transformed[2, :]
    print("[DEBUG] Z min/max:", z_vals.min(), z_vals.max())

    # Optional: count how many points are in front of the camera
    print("[DEBUG] Points in front of camera:", np.sum(z_vals > 0), "/", len(z_vals))

    img = draw_detections(rgb, pred_rot, pred_trans, model_points, K, color=(255, 0, 0))
    img = Image.fromarray(np.uint8(img))
    img.save(save_path)
    prediction = Image.open(save_path)
    
    # concat side by side in PIL
    rgb = Image.fromarray(np.uint8(rgb))
    img = np.array(img)
    concat = Image.new('RGB', (img.shape[1] + prediction.size[0], img.shape[0]))
    concat.paste(rgb, (0, 0))
    concat.paste(prediction, (img.shape[1], 0))
    return concat


def _get_template(path, cfg, tem_index=1):
    rgb_path = os.path.join(path, 'rgb_'+str(tem_index)+'.png')
    mask_path = os.path.join(path, 'mask_'+str(tem_index)+'.png')
    xyz_path = os.path.join(path, 'xyz_'+str(tem_index)+'.npy')

    rgb = load_im(rgb_path).astype(np.uint8)
    xyz = np.load(xyz_path).astype(np.float32) / 1000.0  
    mask = load_im(mask_path).astype(np.uint8) == 255

    bbox = get_bbox(mask)
    y1, y2, x1, x2 = bbox
    mask = mask[y1:y2, x1:x2]

    rgb = rgb[:,:,::-1][y1:y2, x1:x2, :]
    if cfg.rgb_mask_flag:
        rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)

    rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
    rgb = rgb_transform(np.array(rgb))

    choose = (mask>0).astype(np.float32).flatten().nonzero()[0]
    if len(choose) <= cfg.n_sample_template_point:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point)
    else:
        choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_template_point, replace=False)
    choose = choose[choose_idx]
    if len(xyz.shape) == 2:
        # If it's a point cloud, just return it directly
        return rgb, xyz, choose
    xyz = xyz[y1:y2, x1:x2, :].reshape((-1, 3))[choose, :]

    rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)
    return rgb, rgb_choose, xyz


def get_templates(path, cfg):
    n_template_view = cfg.n_template_view
    all_tem = []
    all_tem_choose = []
    all_tem_pts = []

    total_nView = 42
    for v in range(n_template_view):
        i = int(total_nView / n_template_view * v)
        tem, tem_choose, tem_pts = _get_template(path, cfg, i)
        all_tem.append(torch.FloatTensor(tem).unsqueeze(0).cuda())
        all_tem_choose.append(torch.IntTensor(tem_choose).long().unsqueeze(0).cuda())
        all_tem_pts.append(torch.FloatTensor(tem_pts).unsqueeze(0).cuda())
    return all_tem, all_tem_pts, all_tem_choose


def get_test_data(rgb_path, depth_path, cam_path, cad_path, seg_path, det_score_thresh, cfg):
    dets = []
    with open(seg_path) as f:
        dets_ = json.load(f) # keys: scene_id, image_id, category_id, bbox, score, segmentation
    for det in dets_:
        if det['score'] > det_score_thresh:
            dets.append(det)
    del dets_

    cam_info = json.load(open(cam_path))
    
    K = np.array(cam_info['cam_K']).reshape(3, 3)
    print("[DEBUG] Camera intrinsics K:\n", K)
    original_width, original_height = cam_info.get("width", 640), cam_info.get("height", 480)
    new_width, new_height = cfg.img_size, cfg.img_size  # assume square resize (e.g., 224x224 or 512x512)

    scale_x = new_width / original_width
    scale_y = new_height / original_height

    K[0, 0] *= scale_x
    K[0, 2] *= scale_x
    K[1, 1] *= scale_y
    K[1, 2] *= scale_y

    print("[DEBUG] Scaled camera intrinsics K:\n", K)
    


    # rgb_path = "/workspace/USdata/rgb/image0.png" 
    # # depth_path = "/workspace/USdata/depth/depth0.png"
    # rgb_path = f"/workspace/USdata/rgb/image{cfg.frame_idx}.png"
    # print("RGB PATH : ", rgb_path)
    # depth_path = f"/workspace/USdata/depth/depth{cfg.frame_idx}.png"
    # cfg.frame_idx += 1

    whole_image = load_im(rgb_path).astype(np.uint8)
    if len(whole_image.shape)==2:
        whole_image = np.concatenate([whole_image[:,:,None], whole_image[:,:,None], whole_image[:,:,None]], axis=2)
    whole_depth = load_im(depth_path).astype(np.float32) / 1000.0 #* cam_info['depth_scale'] / 1000.0
    if len(whole_depth.shape) == 3:
        whole_depth = cv2.cvtColor(whole_depth, cv2.COLOR_BGR2GRAY)
    whole_pts = get_point_cloud_from_depth(whole_depth, K)
    print("[DEBUG] cam_info['depth_scale'] =", cam_info['depth_scale'])
    print("[DEBUG] depth image min/max =", np.min(whole_depth), np.max(whole_depth))
    print("[DEBUG] whole_pts =", np.min(whole_pts), np.max(whole_pts))

    mesh = trimesh.load_mesh(cad_path)
    print("Bounding box extents (units unknown):", mesh.bounding_box.extents)

    model_points = mesh.sample(cfg.n_sample_model_point).astype(np.float32) / 1000.0
    print(f"[DEBUG] Sampled {cfg.n_sample_model_point} model points")
    print(f"[DEBUG] Model point cloud size (mean distance from origin): {np.mean(np.linalg.norm(model_points, axis=1)):.4f} m")
    print(f"[DEBUG] Model points min/max norm: {np.min(np.linalg.norm(model_points, axis=1)):.4f} / {np.max(np.linalg.norm(model_points, axis=1)):.4f} m")
    print("[DEBUG] model point cloud size (m):", np.mean(np.linalg.norm(model_points, axis=1)))

    radius = np.max(np.linalg.norm(model_points, axis=1))

    all_rgb = []
    all_cloud = []
    all_rgb_choose = []
    all_score = []
    all_dets = []

    for inst in dets:
        # mask
        # _, h,w = seg['size']
        
        seg = inst['segmentation']
        score = inst['score']

        size = seg['size']
        if len(size) == 3:
            size = size[1:]
        h, w = size

        try:
            if isinstance(seg['counts'], list):
                # Fix: Wrap in a list for frPyObjects, not seg itself!
                rle = cocomask.frPyObjects([{
                    'counts': seg['counts'],
                    'size': [h, w]
                }], h, w)
                rle = cocomask.merge(rle)
            else:
                rle = {
                    'counts': seg['counts'],
                    'size': [h, w]
                }

            mask = cocomask.decode(rle)
            if len(mask.shape) == 3:
                mask = mask[:, :, 0]
            print(f"[DEBUG] Mask decoded with shape: {mask.shape}")
            print(f"[DEBUG] Nonzero in mask: {np.sum(mask)}")
            print(f"[DEBUG] Depth valid pixels: {np.sum(whole_depth > 0)}")
        except Exception as e:
            print(f"[WARNING] Failed to decode RLE for detection: {e}")
            continue

        # try:
        #     rle = cocomask.frPyObjects(seg, h, w)
        # except:
        #     rle = seg
        # mask = cocomask.decode(rle)

        mask = np.logical_and(mask > 0, whole_depth > 0)
        #print(f"[DEBUG] Mask after AND with depth > 0: {np.sum(mask)}")

        if np.sum(mask) > 16: #32:
            bbox = get_bbox(mask)
            y1, y2, x1, x2 = bbox
        else:
            #print("[WARNING] Mask discarded: too few valid pixels")
            continue
        mask = mask[y1:y2, x1:x2]
        choose = mask.astype(np.float32).flatten().nonzero()[0]
        #print(f"[DEBUG] choose count: {len(choose)}")

        # pts
        cloud = whole_pts.copy()[y1:y2, x1:x2, :].reshape(-1, 3)[choose, :]
        #print(f"[DEBUG] cloud shape: {cloud.shape}")
        center = np.mean(cloud, axis=0)
        tmp_cloud = cloud - center[None, :]
        print(f"[DEBUG] cloud shape: {cloud.shape}")
        print(f"[DEBUG] cloud sample point: {cloud[0] if len(cloud) > 0 else 'EMPTY'}")
        print(f"[DEBUG] cloud norms (min/max): {np.min(np.linalg.norm(cloud, axis=1)):.3f} / {np.max(np.linalg.norm(cloud, axis=1)):.3f}")
        print(f"[DEBUG] model radius * 1.2 = {radius * 1.2:.4f}")

        flag = np.linalg.norm(tmp_cloud, axis=1) < radius * 1.2
        #print(f"[DEBUG] points after radius filter: {np.sum(flag)}")

        if np.sum(flag) < 1:
            continue
        choose = choose[flag]
        cloud = cloud[flag]

        if len(choose) <= cfg.n_sample_observed_point:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point)
        else:
            choose_idx = np.random.choice(np.arange(len(choose)), cfg.n_sample_observed_point, replace=False)
        choose = choose[choose_idx]
        cloud = cloud[choose_idx]

        # rgb
        # Extract the cropped region from the original image
        rgb = whole_image.copy()[y1:y2, x1:x2, :][:, :, ::-1]
        mask_crop = mask[y1:y2, x1:x2]
        # Check for empty crops
        if rgb.size == 0 or mask_crop.size == 0 or rgb.shape[0] == 0 or rgb.shape[1] == 0:
            print("[WARNING] Skipping detection due to invalid crop dimensions")
            continue

        # Apply the mask if enabled in config
        if cfg.rgb_mask_flag:
            # Resize mask if it doesn't match
            if rgb.shape[:2] != mask_crop.shape:
                print("[WARNING] Shape mismatch: resizing mask to match RGB crop")
                try:
                    mask_crop = cv2.resize(mask_crop.astype(np.uint8), (rgb.shape[1], rgb.shape[0]), interpolation=cv2.INTER_NEAREST)
                except Exception as e:
                    print(f"[ERROR] Failed resizing mask: {e}")
                    continue

            rgb = rgb * (mask_crop[:, :, None] > 0).astype(np.uint8)
        # Resize and normalize
        rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        rgb = rgb_transform(np.array(rgb))
        # rgb = whole_image.copy()[y1:y2, x1:x2, :][:,:,::-1]
        # if cfg.rgb_mask_flag:
        #     #resized_mask = cv2.resize(mask.astype(np.uint8), (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_NEAREST)
        #     #rgb = rgb * (resized_mask[:, :, None] > 0).astype(np.uint8)
        #     rgb = rgb * (mask[:,:,None]>0).astype(np.uint8)
        # rgb = cv2.resize(rgb, (cfg.img_size, cfg.img_size), interpolation=cv2.INTER_LINEAR)
        # rgb = rgb_transform(np.array(rgb))
        rgb_choose = get_resize_rgb_choose(choose, [y1, y2, x1, x2], cfg.img_size)

        all_rgb.append(torch.FloatTensor(rgb))
        all_cloud.append(torch.FloatTensor(cloud))
        all_rgb_choose.append(torch.IntTensor(rgb_choose).long())
        all_score.append(score)
        all_dets.append(inst)
        print(f"[INFO] Final number of valid detections: {len(all_cloud)}")

    print(f"[INFO] Final number of valid detections: {len(all_cloud)}")
    ret_dict = {}
    ret_dict['pts'] = torch.stack(all_cloud).cuda()
    ret_dict['rgb'] = torch.stack(all_rgb).cuda()
    ret_dict['rgb_choose'] = torch.stack(all_rgb_choose).cuda()
    ret_dict['score'] = torch.FloatTensor(all_score).cuda()

    ninstance = ret_dict['pts'].size(0)
    ret_dict['model'] = torch.FloatTensor(model_points).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    ret_dict['K'] = torch.FloatTensor(K).unsqueeze(0).repeat(ninstance, 1, 1).cuda()
    return ret_dict, whole_image, whole_pts.reshape(-1, 3), model_points, all_dets



if __name__ == "__main__":
    cfg = init()


    random.seed(cfg.rd_seed)
    torch.manual_seed(cfg.rd_seed)

    # model
    print("=> creating model ...")
    MODEL = importlib.import_module(cfg.model_name)
    model = MODEL.Net(cfg.model)
    model = model.cuda()
    model.eval()
    checkpoint = os.path.join(os.path.dirname((os.path.abspath(__file__))), 'checkpoints', 'sam-6d-pem-base.pth')
    #gorilla.solver.load_checkpoint(model=model, filename=checkpoint)
    model.load_state_dict(torch.load(checkpoint, map_location='cuda')['model'])

    print("=> extracting templates ...")
    tem_path = os.path.join(cfg.output_dir, 'templates')
    all_tem, all_tem_pts, all_tem_choose = get_templates(tem_path, cfg.test_dataset)
    with torch.no_grad():
        all_tem_pts, all_tem_feat = model.feature_extraction.get_obj_feats(all_tem, all_tem_pts, all_tem_choose)

    print("=> loading input data ...")
    frame_idx = cfg.frame_idx
    rgb_path = f"/workspace/USdata/rgb/image{frame_idx}.png"
    depth_path = f"/workspace/USdata/depth/depth{frame_idx}.png"
    input_data, img, whole_pts, model_points, detections = get_test_data(
        rgb_path, depth_path, cfg.cam_path, cfg.cad_path, cfg.seg_path, 
        cfg.det_score_thresh, cfg.test_dataset
    )
    ninstance = input_data['pts'].size(0)
    
    print("=> running model ...")
    with torch.no_grad():
        input_data['dense_po'] = all_tem_pts.repeat(ninstance,1,1)
        input_data['dense_fo'] = all_tem_feat.repeat(ninstance,1,1)
        out = model(input_data)

    # if 'pred_pose_score' in out.keys():
    #     pose_scores = out['pred_pose_score'] * out['score']
    # else:
    #     pose_scores = out['score']
    pose_scores = out['score']
    print("[DEBUG] out['score']:", out['score'])
    if 'pred_pose_score' in out:
        print("[DEBUG] out['pred_pose_score']:", out['pred_pose_score'])
        print("[DEBUG] out['score']:", out['score'])

    pose_scores = pose_scores.detach().cpu().numpy()
    pred_rot = out['pred_R'].detach().cpu().numpy()
    pred_trans = out['pred_t'].detach().cpu().numpy() * 1000
    

    print("[DEBUG] Predicted R:", pred_rot)
    print("[DEBUG] Predicted t:", pred_trans)


    print("=> saving results ...")
    os.makedirs(f"{cfg.output_dir}/sam6d_results", exist_ok=True)
    for idx, det in enumerate(detections):
        detections[idx]['score'] = float(pose_scores[idx])
        detections[idx]['R'] = list(pred_rot[idx].tolist())
        detections[idx]['t'] = list(pred_trans[idx].tolist())

    with open(os.path.join(f"{cfg.output_dir}/sam6d_results", 'detection_pem.json'), "w") as f:
        json.dump(detections, f)

    print("=> visualizating ...")

    # Translation sanity check
    print("[DEBUG] Predicted translation (t) values:")
    for i, t in enumerate(pred_trans):
        print(f"  Instance {i}: t = {t}, norm = {np.linalg.norm(t):.2f}")

    # Model point cloud scale
    model_point_norms = np.linalg.norm(model_points, axis=1)
    print(f"[DEBUG] Model point cloud stats:")
    print(f"  Mean distance from origin: {np.mean(model_point_norms):.3f}")
    print(f"  Min/Max distance: {np.min(model_point_norms):.3f} / {np.max(model_point_norms):.3f}")

    # Are translations in mm or m?
    print(f"[DEBUG] Translation magnitude range suggests units in {'millimeters' if np.mean(np.linalg.norm(pred_trans, axis=1)) > 100 else 'meters'}")

    # Should model_points be scaled?
    if np.mean(model_point_norms) < 1:
        print("[DEBUG] Model points are small - likely in meters.")
    else:
        print("[DEBUG] Model points are in mm or scaled correctly.")

    # Reminder

    save_path = os.path.join(f"{cfg.output_dir}/sam6d_results", 'vis_pem.png')
    # valid_masks = pose_scores.max()
    # if np.sum(valid_masks) == 0:
    #     print("[WARNING] No valid poses to visualize")
    # else:
    print("[DEBUG] Pose scores:", pose_scores)
    best_idx = np.argmax(pose_scores)

    if pose_scores[best_idx] <= 0:
        print("[WARNING] No valid poses to visualize")
    else:
        print("model_points shape:", model_points.shape)

        K = input_data['K'].detach().cpu().numpy()[best_idx]
        #vis_img = visualize(img, pred_rot[best_idx], pred_trans[best_idx], model_points * 1000, K, save_path)
        #pred_trans[best_idx] /= 100.0
        #pred_trans = pred_trans / 1000.0
        vis_img = visualize(
            img,
            pred_rot[best_idx][np.newaxis],
            pred_trans[best_idx][np.newaxis],
            model_points* 1000,
            K[np.newaxis],
            save_path
        )
        vis_img.save(save_path)
    # K = input_data['K'].detach().cpu().numpy()[valid_masks]
    # vis_img = visualize(img, pred_rot[valid_masks], pred_trans[valid_masks], model_points * 1000, K, save_path)
    # vis_img.save(save_path)

    import gc
    gc.collect()
    torch.cuda.empty_cache()

